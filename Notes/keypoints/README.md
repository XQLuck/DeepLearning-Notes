# 深度学习要点梳理和个人理解

这里记录下对深度学习某些知识和内容的探究、梳理，以及个人理解。



## 写在前面

:notes: 有一些学习要点的笔记和梳理我有随记在「幕布」和「有道云笔记」平台，挺杂乱的，但值得看一看：

- [01-机器学习&深度学习要点小梳理（含资料推荐）](https://mubu.com/doc/2E8oghDU78)
- [02-卷积神经网络CNN探究(含反卷积、转置卷积、空洞卷积、上采样、下采样)](https://mubu.com/doc/3LbSzN4z-8)
- [03-CNN网络模型演进(LetNet、AleNet、VGGNet、NIN、GoogleNet、ResNet等)](https://mubu.com/doc/2BFlc9r-B8)
- [04-图像语义分割(含FCN、UNet、SegNet、PSPNet、Deeplabv1&v2&v3等)](https://mubu.com/doc/1OEfnuDXAc)
- [数据预处理：中心化（又叫零均值化）和标准化（又叫归一化）](https://mubu.com/doc/2qC5u7sGw8)
- [对softmax，softmax loss和cross entropy的理解](https://mubu.com/doc/T0NtYmnFc)
- ……

里面的内容有的会继续补充，另外，有新作的要点和内容，我会以分享链接的形式继续更新在上面，不过以上的分享链接也有可能指不定哪天被我取消了，不过关注该仓库即可，日后有时间整理成文至此。

注：对于在「幕布」上作的内容，有的我有导出成了 HTML 文件，可以在该文目录下的 html 文件夹下找到，但在 GitHub 上直接打开 HTML 文件只能看到源代码，看不到显示效果，如果想要看到正常文件内容，可以打开  [htmlpreview](https://htmlpreview.github.io/)，然后复制 HTML 文件的地址链接过去即可看到内容。



## 1. 深度学习之笔记梳理

### (1) 我的理解：神经网络参数改变过程？

见：[我的理解：神经网络参数改变过程？.md](./我的理解：神经网络参数改变过程？.md)





## 2. 深度学习之基本数学

### (1) 什么是标准差和方差？

推荐阅读：[标准差和方差](https://www.shuxuele.com/data/standard-deviation.html)

总结：当数据比较分散时，标准差也比较大，即**数据越分数，标准差也越大**。另外，试着体会这句话：**“差”的意思是离正常有多远。**



### (2) 什么是正太分布？

推荐阅读：[正态分布为什么常见？ - 阮一峰的网络日志](http://www.ruanyifeng.com/blog/2017/08/normal-distribution.html)

统计学里面，正态分布（normal distribution）最常见。男女身高、寿命、血压、考试成绩、测量误差等等，都属于正态分布。

。。。。。。

比如，财富的分布就是不对称的，富人的有钱程度（可能比平均值高出上万倍），远远超出穷人的贫穷程度（平均值的十分之一就是赤贫了），即财富分布曲线有右侧的长尾。相比来说，身高的差异就小得多，最高和最矮的人与平均身高的差距，都在30%多。

这是为什么呢，财富明明也受到多种因素的影响，怎么就不是正态分布呢？

原来，正态分布只适合各种因素累加的情况，如果这些因素不是彼此独立的，会互相加强影响，那么就不是正态分布了。一个人是否能够挣大钱，由多种因素决定：

- 家庭
- 教育
- 运气
- 工作
- ...

这些因素都不是独立的，会彼此加强。如果出生在上层家庭，那么你就有更大的机会接受良好的教育、找到高薪的工作、遇见好机会，反之亦然。也就是说，这不是 1 + 1 = 2 的效果，而是 1 + 1 > 2。

统计学家发现，如果各种因素对结果的影响不是相加，而是相乘，那么最终结果不是正态分布，而是[对数正态分布](https://baike.baidu.com/item/%E5%AF%B9%E6%95%B0%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83)（log normal distribution），即`x`的对数值`log(x)`满足正态分布。

这就是说，财富的对数值满足正态分布。如果平均财富是10,000元，那么1000元～10,000元之间的穷人（比平均值低一个数量级，宽度为9000）与10,000元~100,000元之间的富人（比平均值高一个数量级，宽度为90,000）人数一样多。因此，财富曲线左侧的范围比较窄，右侧出现长尾。



## 3. 深度学习之CV有关问题

### (1) top5错误率

参考：[什么是图像分类的Top-5错误率？ - 知乎](https://www.zhihu.com/question/36463511)

- 回答1：imagenet图像通常有1000个可能的类别，对每幅图像你可以猜5次结果(即同时预测5个类别标签)，当其中有任何一次预测对了，结果都算对，当5次全都错了的时候，才算预测错误，这时候的分类错误率就叫top5错误率。

- 回答2：翻译一下和Yong Pan答案是一样的，top1就是你预测的label取最后概率向量里面最大的那一个作为预测结果，你的预测结果中概率最大的那个类必须是正确类别才算预测正确。而top5就是最后概率向量最大的前五名中出现了正确概率即为预测正确。

### (2) 条件随机场CRF在语义分割上的应用

对于每个像素位置 i 具有隐变量 (这里隐变量就是像素的真实类别标签，如果预测结果有 21 类，则 (i∈1,2,..,21)，还有对应的观测值 yi (即像素点对应的颜色值)。以像素为节点，像素与像素间的关系作为边，构成了一个“条件随机场”（ Conditional Random Field，简称 CRF）。通过观测变量 yi 来推测像素位置 i 对应的类别标签 xi。条件随机场示意图如下：

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190303194042.png)

参考：[语义分割论文-DeepLab系列](http://t.cn/E2z2Bs6)

很多以深度学习为框架的图像语义分割系统都使用了一种叫做 “条件随机场”（ Conditional Random Field，简称 **CRF**）的技术作为输出结果的优化后处理手段。其实类似技术种类较多，比如还有马尔科夫随机场 (MRF) 和高斯条件随机场 (G-CRF) 用的也比较多，但原理都较为类似。

简单来介绍一下 “条件随机场” 的概念。

FCN 是像素到像素的影射，所以最终输出的图片上每一个像素都是标注了分类的，将这些分类简单地看成是不同的变量，每个像素都和其他像素之间建立一种连接，连接就是相互间的关系。于是就会得到一个 “完全图”：

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190303194723.png)



上图是以 4x6 大小的图像像素阵列表示的简易版。那么在全链接的 CRF 模型中，有一个对应的能量函数：

![](https://img-1256179949.cos.ap-shanghai.myqcloud.com/20190303194936.png)

嗯，不要问我这个公式里各种符号是啥，我看不懂。但是我知道这个公式是干嘛滴：

其中等号右边第一个一元项，表示像素对应的语义类别，其类别可以由 FCN 或者其他语义分割模型的预测结果得到；而第二项为二元项，二元项可将像素之间的语义联系 / 关系考虑进去。

这么说太抽象，举个简单的例子，“天空”和 “鸟” 这样的像素在物理空间是相邻的概率，应该要比 “天空” 和 “鱼” 这样像素相邻的概率大，那么天空的边缘就更应该判断为鸟而不是鱼（从概率的角度）。

通过对这个能量函数优化求解，把明显不符合事实识别判断剔除，替换成合理的解释，得到对 FCN 的图像语义预测结果的优化，生成最终的语义分割结果。

来源：[十分钟看懂图像语义分割技术 | 雷锋网](https://www.leiphone.com/news/201705/YbRHBVIjhqVBP0X5.html)



---

*update：2019-03-03* 